{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FIT5196 Assessment 1\n",
    "#### Student Name:Aishwarya Srirangam Murali \n",
    "#### Student ID: 29999715\n",
    "\n",
    "Date: 16/09/2020\n",
    "\n",
    "Libraries used:\n",
    "* re(for regular expression, included in Anaconda Python 3.6) \n",
    "* os (For interactig with the files and operation systems)\n",
    "* langid (for language detection and processing)\n",
    "* pandas 0.19.2 (for data frame, included in Anaconda Python 3.6) \n",
    "* nltk 3.2.2 (Natural Language Toolkit, included in Anaconda Python 3.6)\n",
    "* nltk.collocations (for finding bigrams, included in Anaconda Python 3.6)\n",
    "* nltk.tokenize (for tokenization, included in Anaconda Python 3.6)\n",
    "* nltk.corpus (for stop words, not included in Anaconda, `nltk.download('stopwords')` provided)\n",
    "* nltk.stem (for stemming)\n",
    "* nltk.util (for ngrams)\n",
    "* numpy (For arrays and matrices)\n",
    "\n",
    "## 1. Introduction\n",
    "This assignment comprises the extraction of data from excel file. We are given excel file which contains tweets where we are to extract the tweet and the text associated with the tweet. After we have extracted the tweet we are to filter the english tweets. Next we have to generate the unigrams, bigrams and the sample count. \n",
    "\n",
    "More details for each task will be given in the following sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.  Importing all the libraries \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import numpy as np\n",
    "import langid\n",
    "from tqdm import tqdm_notebook\n",
    "from collections import Counter, OrderedDict\n",
    "from itertools import chain\n",
    "from nltk.tokenize import RegexpTokenizer,  MWETokenizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.util import ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.  Importing all the libraries \n",
    "* Here, we are first reading the excel file \n",
    "* Dropping all the empty rows and columns\n",
    "* Extracting the text, id and created at \n",
    "* Concatenate all of it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Yes, all networks need to stop covering Trump ...</td>\n",
       "      <td>1241579835371278080</td>\n",
       "      <td>2020-03-22T04:17:54.000Z</td>\n",
       "      <td>2020-03-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Oga Sir! Directive for closure of all public g...</td>\n",
       "      <td>1241624080391994880</td>\n",
       "      <td>2020-03-22T07:13:43.000Z</td>\n",
       "      <td>2020-03-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Stay at home so the js virus doesn't spread\\n\\...</td>\n",
       "      <td>1241680036803087872</td>\n",
       "      <td>2020-03-22T10:56:04.000Z</td>\n",
       "      <td>2020-03-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Médicos y personal sanitario infectado: 3.475 ...</td>\n",
       "      <td>1241713141941240064</td>\n",
       "      <td>2020-03-22T13:07:37.000Z</td>\n",
       "      <td>2020-03-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Chinese scientists dismiss claims Covid19 pati...</td>\n",
       "      <td>1241605146758569984</td>\n",
       "      <td>2020-03-22T05:58:29.000Z</td>\n",
       "      <td>2020-03-22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "0                                               text                   id  \\\n",
       "0  Yes, all networks need to stop covering Trump ...  1241579835371278080   \n",
       "1  Oga Sir! Directive for closure of all public g...  1241624080391994880   \n",
       "2  Stay at home so the js virus doesn't spread\\n\\...  1241680036803087872   \n",
       "3  Médicos y personal sanitario infectado: 3.475 ...  1241713141941240064   \n",
       "4  Chinese scientists dismiss claims Covid19 pati...  1241605146758569984   \n",
       "\n",
       "0                created_at        date  \n",
       "0  2020-03-22T04:17:54.000Z  2020-03-22  \n",
       "1  2020-03-22T07:13:43.000Z  2020-03-22  \n",
       "2  2020-03-22T10:56:04.000Z  2020-03-22  \n",
       "3  2020-03-22T13:07:37.000Z  2020-03-22  \n",
       "4  2020-03-22T05:58:29.000Z  2020-03-22  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Reading the excel file \n",
    "data = pd.read_excel('part2/Excel file part2.xlsx', sheet_name=None)\n",
    "\n",
    "dfs = []\n",
    "\n",
    "for date in data.keys():\n",
    "    df =  data[date].copy()\n",
    "    #Dropping all the rows and columns with null values \n",
    "    df.dropna(axis=1, how='all', inplace=True) #drop empty cols \n",
    "    df.dropna(axis=0, how='all', inplace=True) #drop empty rows\n",
    "    df.reset_index(inplace=True, drop=True)\n",
    "    if ('text' in df.columns) and ('id' in df.columns) and ('created_at' in df.columns):\n",
    "        pass\n",
    "    else:\n",
    "        df.columns = df.iloc[0, :]\n",
    "        df = df.iloc[1:]\n",
    "    assert ('text' in df.columns) and ('id' in df.columns) and ('created_at' in df.columns) and (len(df.columns)==3)\n",
    "    df['date'] = date\n",
    "    dfs.append(df)\n",
    "\n",
    "df = pd.concat(dfs, ignore_index=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162000\n"
     ]
    }
   ],
   "source": [
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Writing a function for langid - classifying the language \n",
    "def classify_lang(x):\n",
    "    try:\n",
    "        return langid.classify(x)[0]\n",
    "    except:\n",
    "        return '-1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "550145bfa82c4077b351fcb26505784b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=162000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>date</th>\n",
       "      <th>lang</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Yes, all networks need to stop covering Trump ...</td>\n",
       "      <td>1241579835371278080</td>\n",
       "      <td>2020-03-22T04:17:54.000Z</td>\n",
       "      <td>2020-03-22</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Oga Sir! Directive for closure of all public g...</td>\n",
       "      <td>1241624080391994880</td>\n",
       "      <td>2020-03-22T07:13:43.000Z</td>\n",
       "      <td>2020-03-22</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Stay at home so the js virus doesn't spread\\n\\...</td>\n",
       "      <td>1241680036803087872</td>\n",
       "      <td>2020-03-22T10:56:04.000Z</td>\n",
       "      <td>2020-03-22</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Médicos y personal sanitario infectado: 3.475 ...</td>\n",
       "      <td>1241713141941240064</td>\n",
       "      <td>2020-03-22T13:07:37.000Z</td>\n",
       "      <td>2020-03-22</td>\n",
       "      <td>es</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Chinese scientists dismiss claims Covid19 pati...</td>\n",
       "      <td>1241605146758569984</td>\n",
       "      <td>2020-03-22T05:58:29.000Z</td>\n",
       "      <td>2020-03-22</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "0                                               text                   id  \\\n",
       "0  Yes, all networks need to stop covering Trump ...  1241579835371278080   \n",
       "1  Oga Sir! Directive for closure of all public g...  1241624080391994880   \n",
       "2  Stay at home so the js virus doesn't spread\\n\\...  1241680036803087872   \n",
       "3  Médicos y personal sanitario infectado: 3.475 ...  1241713141941240064   \n",
       "4  Chinese scientists dismiss claims Covid19 pati...  1241605146758569984   \n",
       "\n",
       "0                created_at        date lang  \n",
       "0  2020-03-22T04:17:54.000Z  2020-03-22   en  \n",
       "1  2020-03-22T07:13:43.000Z  2020-03-22   en  \n",
       "2  2020-03-22T10:56:04.000Z  2020-03-22   en  \n",
       "3  2020-03-22T13:07:37.000Z  2020-03-22   es  \n",
       "4  2020-03-22T05:58:29.000Z  2020-03-22   en  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['lang'] = [classify_lang(x) for x in tqdm_notebook(df['text'])]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Filtering non english tweets \n",
    "* Dropping all the tweets that are not english"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(162000, 5)\n",
      "(91733, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>date</th>\n",
       "      <th>lang</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Yes, all networks need to stop covering Trump ...</td>\n",
       "      <td>1241579835371278080</td>\n",
       "      <td>2020-03-22T04:17:54.000Z</td>\n",
       "      <td>2020-03-22</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Oga Sir! Directive for closure of all public g...</td>\n",
       "      <td>1241624080391994880</td>\n",
       "      <td>2020-03-22T07:13:43.000Z</td>\n",
       "      <td>2020-03-22</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Stay at home so the js virus doesn't spread\\n\\...</td>\n",
       "      <td>1241680036803087872</td>\n",
       "      <td>2020-03-22T10:56:04.000Z</td>\n",
       "      <td>2020-03-22</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Chinese scientists dismiss claims Covid19 pati...</td>\n",
       "      <td>1241605146758569984</td>\n",
       "      <td>2020-03-22T05:58:29.000Z</td>\n",
       "      <td>2020-03-22</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>We want to make this a special mother’s day! A...</td>\n",
       "      <td>1241666046681140992</td>\n",
       "      <td>2020-03-22T10:00:29.000Z</td>\n",
       "      <td>2020-03-22</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "0                                               text                   id  \\\n",
       "0  Yes, all networks need to stop covering Trump ...  1241579835371278080   \n",
       "1  Oga Sir! Directive for closure of all public g...  1241624080391994880   \n",
       "2  Stay at home so the js virus doesn't spread\\n\\...  1241680036803087872   \n",
       "4  Chinese scientists dismiss claims Covid19 pati...  1241605146758569984   \n",
       "6  We want to make this a special mother’s day! A...  1241666046681140992   \n",
       "\n",
       "0                created_at        date lang  \n",
       "0  2020-03-22T04:17:54.000Z  2020-03-22   en  \n",
       "1  2020-03-22T07:13:43.000Z  2020-03-22   en  \n",
       "2  2020-03-22T10:56:04.000Z  2020-03-22   en  \n",
       "4  2020-03-22T05:58:29.000Z  2020-03-22   en  \n",
       "6  2020-03-22T10:00:29.000Z  2020-03-22   en  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Keep english tweets only and dropping the others\n",
    "print(df.shape)\n",
    "df = df[df['lang'] == 'en']\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Concatenating and defining functions \n",
    "* First, concatenating all the tweets \n",
    "* We are using the regular expression to tokenize \n",
    "* Adding the top 200 bigrams to the vocabulary \n",
    "* Stemming using porter stemmer\n",
    "* Removing the stop words \n",
    "* Dropping the words which are not in the vocabulary \n",
    "* Writing it to the file \n",
    "* Reading the file \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Concatenating all the tweets for the date \n",
    "raw_data = {}\n",
    "for date in df['date'].unique():\n",
    "    raw_data[date] = \" \".join(df[df['date']==date]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using the regular expression as given to get the tokenised data \n",
    "def tokenize_data(data):\n",
    "    tokenizer = RegexpTokenizer(r'[a-zA-Z]{2,}')\n",
    "    #converting everything to lower case \n",
    "    tokenized_data = dict((date, tokenizer.tokenize(text.lower())) for date, text in data.items())\n",
    "    return tokenized_data\n",
    "\n",
    "\n",
    "#Top 200 bigrams to the vocabulary\n",
    "def add_collocations(data):\n",
    "    all_words = list(chain.from_iterable(data.values()))\n",
    "    bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "    bigram_finder = nltk.collocations.BigramCollocationFinder.from_words(all_words)\n",
    "    bigram_finder.apply_freq_filter(20)\n",
    "    bigram_finder.apply_word_filter(lambda w: len(w) < 3)\n",
    "    top_200_bigrams = bigram_finder.nbest(bigram_measures.pmi, 200)\n",
    "    mwetokenizer = MWETokenizer(top_200_bigrams)\n",
    "    colloc_data =  dict((date, mwetokenizer.tokenize(tokens)) for date, tokens in data.items())\n",
    "    return colloc_data\n",
    "\n",
    "\n",
    "#Stemming using Porter stemmer \n",
    "def stem_data(data):\n",
    "    stemmer = PorterStemmer()\n",
    "    stemmed_data = dict((date, np.vectorize(stemmer.stem)(tokens)) for date, tokens in data.items())\n",
    "    return stemmed_data\n",
    "\n",
    "\n",
    "#Removing the stop words and removing the tokens where the length is less than 3 \n",
    "def make_vocab(data, stop_words=[]):\n",
    "    unique_tokens = np.concatenate([np.unique(tokens) for date, tokens in data.items()]).flatten()\n",
    "    counter = Counter(unique_tokens)\n",
    "    vocab = list(counter)\n",
    "    vocab = [token for token in vocab if counter[token]>5] #Removing rare tokens\n",
    "    vocab = [token for token in vocab if counter[token]<60] #context-dependent stop tokens\n",
    "    vocab = [token for token in vocab if token not in stop_words] #context-independent stop tokens\n",
    "    vocab = [token for token in vocab if len(token)>3] \n",
    "    return OrderedDict((token, i) for token, i in zip(sorted(vocab), range(len(vocab))))\n",
    "\n",
    "\n",
    "#Dropping the words which are not in vocabulary \n",
    "def filter_based_on_vocab(data, vocab):\n",
    "    filtered_data = {}\n",
    "    for date, tokens in data.items():\n",
    "        filtered_data[date] = list(filter(lambda x: x in vocab, tokens))\n",
    "    return filtered_data\n",
    "\n",
    "\n",
    "#Writing it to the file \n",
    "def write_to_file(data, filepath, join_using=':'):\n",
    "    with open(filepath, 'w') as f:\n",
    "        for k, v in tqdm_notebook(data.items()):\n",
    "            f.write(f'{k}{join_using}{str(v)}\\n')\n",
    "            \n",
    "            \n",
    "#Reading the file     \n",
    "def read_stop_words(filepath):\n",
    "    return list(np.loadtxt(filepath, dtype=str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Functions\n",
    "* Calling all the functions defined above \n",
    "* Writing a functions for n grams \n",
    "* Taking the first 100 for both unigram and bigram \n",
    "* Writing it to the files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86dce91f50054820818d2c3e4fd76b02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=8039), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#Calling all of the functions defined above \n",
    "data = raw_data.copy()\n",
    "data = tokenize_data(data)\n",
    "data = add_collocations(data)\n",
    "data = stem_data(data)\n",
    "stop_words = read_stop_words('part2/stopwords_en.txt')\n",
    "vocab = make_vocab(data, stop_words=stop_words) \n",
    "data = filter_based_on_vocab(data, vocab)\n",
    "\n",
    "\n",
    "write_to_file(vocab, 'part2/29999715_vocab.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Writing a function for n grams \n",
    "def top_k_freq_ngrams(tokens:list, n:int, k:int,):\n",
    "    all_n_grams = ngrams(sequence=tokens, n=n)\n",
    "    freq = Counter(all_n_grams)\n",
    "    k_most_common = freq.most_common(k)\n",
    "    if n == 1:\n",
    "        k_most_common = [(token[0], count) for token, count in k_most_common]\n",
    "    return k_most_common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calling the function written above and taking the first 100 for both unigram and bigram \n",
    "unigrams = dict((date, top_k_freq_ngrams(tokens, n=1, k=100)) for date, tokens in data.items())\n",
    "bigrams = dict((date, top_k_freq_ngrams(tokens, n=2, k=100)) for date, tokens in data.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4dc4bb93cfe4b4f87721a257ddbac28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=81), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d0c13c814ba4a5e81c646c92a8faab4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=81), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#Key of the dict, string of the value and \n",
    "#saving the unigram and bigram file \n",
    "write_to_file(unigrams, 'part2/29999715_100uni.txt')\n",
    "write_to_file(bigrams, 'part2/29999715_100bi.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#List of the documents from string of words \n",
    "corpus = [\" \".join(tokens) for date, tokens in data.items()]\n",
    "#The dates \n",
    "docids = [date for date, tokens in data.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sparse matrix \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "053df09cb219434a858434fdb969a4d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=81), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "countvec_dict = dict((date, \",\".join(list(map(str, vec)))) for date, vec in zip(docids, X.toarray()))\n",
    "write_to_file(countvec_dict, './part2/29999715_countVec.txt', join_using=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(vectorizer.get_feature_names()) == len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Reference\n",
    "1. re — Regular expression operations — Python 3.8.6rc1 documentation. (2020). Retrieved 16 September 2020, from https://docs.python.org/3/library/re.html\n",
    "2. os — Miscellaneous operating system interfaces — Python 3.8.6rc1 documentation. (2020). Retrieved 16 September 2020, from https://docs.python.org/3/library/os.html\n",
    "3. langid. (2020). Retrieved 16 September 2020, from https://pypi.org/project/langid/1.1dev/\n",
    "4. xmltodict. (2020). Retrieved 16 September 2020, from https://pypi.org/project/xmltodict/\n",
    "5. pandas - Python Data Analysis Library. (2020). Retrieved 16 September 2020, from https://pandas.pydata.org/\n",
    "6. nltk. (2020). Retrieved 16 September 2020, from https://pypi.org/project/nltk/\n",
    "7. NumPy. (2020). Retrieved 16 September 2020, from https://numpy.org/    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    
    "Libraries used:\n",
    "* re 2.2.1 (for regular expression, included in Anaconda Python 3.6) \n",
    "* os (For interactig with the files and operation systems)\n",
    "* langid (for language detection and processing)\n",
    "\n",
    "## 1. Introduction\n",
    "This assignment comprises the extraction of data from semi-structured files. We are given text files which contains tweets where we are to extract the date,id of the tweet and the text. After we have extracted the tweet we are to write it into XML format. \n",
    "\n",
    "More details for each task will be given in the following sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.  Import libraries \n",
    "* Importing os for reading the files \n",
    "* Using langid for filtering english language \n",
    "* re for regular expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import langid\n",
    "import re\n",
    "import xmltodict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Loading the files\n",
    "* We are loading all the files in part 1 which ends with .txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2413\n"
     ]
    }
   ],
   "source": [
    "path = './part1/'\n",
    "text_files = [f for f in os.listdir(path) if f.endswith('.txt')]\n",
    "print(len(text_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Cleaning the data\n",
    "* We are first cleaning the data \n",
    "* Fixing the issue with emojis \n",
    "* Quotes which were not closed are fixed and completing the quotes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaning the text which are not completed \n",
    "def clean_text(text):\n",
    "    return (text.replace('ï', '').replace('¸', '').replace('', '')\n",
    "            .replace('â', '')\n",
    "            .replace('', ''))\n",
    "\n",
    "#Fixing the emojis and the special characters \n",
    "def fix_emojis(t):\n",
    "    i = 0\n",
    "    to_change = None\n",
    "    changes = []\n",
    "    while i<=(len(t)):\n",
    "        if t[i:i+2] == '\\\\u':\n",
    "            if to_change is None:to_change=t[i: i+6]\n",
    "            else: to_change += t[i: i+6]\n",
    "            i +=6\n",
    "        else: \n",
    "            if to_change is not None:\n",
    "                s = to_change.encode('utf-8').decode('unicode-escape').encode('utf-16', 'surrogatepass').decode('utf-16')\n",
    "                changes.append((to_change, s))\n",
    "                to_change=None\n",
    "            i = i+1\n",
    "    for o, n in sorted(changes, key=lambda x: len(x[1]), reverse=True):\n",
    "        t = t.replace(o, n)\n",
    "    return t\n",
    "\n",
    "#Some of the quotes were not completely closed. Hence, completing the quotes \n",
    "def fix_unclosed_quotes(t):\n",
    "    for i in reversed(t):\n",
    "        if i=='”' or i=='\"':\n",
    "            return t\n",
    "        elif i=='“':\n",
    "            return t+'\"'\n",
    "    return t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Parsing the data \n",
    "1. We are using regex for date and id \n",
    "2. Date - Date is in the format of yyyy-mm-dd so we are using \\d{4}-\\d{2}-\\d{2}.*?Z\n",
    "    * \\\" matches the character \" literally (case sensitive)\n",
    "    * \\d matches the digits from 0-9 and {4} matches 4 digits \n",
    "    * .*? matches any character (except for line terminators)\n",
    "    * Z matches the character Z literally (case sensitive)\n",
    "3. For the id we use :\\\"(\\d{2,19})\\\"\n",
    "    * \\d{2,19} matches a digit (equal to [0-9]) since the length of the id is a constant of 19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parsing the tweets using regex \n",
    "def parse_tweet(tweet):\n",
    "    tweet_date = re.findall('\\\"\\d{4}-\\d{2}-\\d{2}.*?Z\\\"', tweet)[0].split('T')[0].strip('\"')\n",
    "    if len(tweet_date)>10: tweet_date = tweet_date[:10]\n",
    "    tweet_id = re.findall(':\\\"(\\d{2,19})\\\"', tweet)[0]\n",
    "    if len(tweet_id)>19: tweet_id = tweet_date[:19]\n",
    "    tweet = re.split('\",\"|},\"', tweet)\n",
    "\n",
    "    tweet_text=None\n",
    "    for part in tweet:\n",
    "        part = part.lstrip('\"')\n",
    "        if part.startswith('text\":'):\n",
    "            tweet_text = part[len('text\":\"'):]\n",
    "            tweet_text = tweet_text.replace('\\\\n', '\\n').replace('\\\\\"', '\"')\n",
    "            if tweet_text[-1] == '\"': tweet_text = tweet_text[:-1]\n",
    "            #calling the emojis function and fixig the tweet\n",
    "            tweet_text = fix_emojis(tweet_text)\n",
    "            #If it comes across any unclosed quotes fixing that as well\n",
    "            tweet_text = fix_unclosed_quotes(tweet_text)\n",
    "    for i in [tweet_date, tweet_id, tweet_text]:\n",
    "        try:\n",
    "            assert not i is None and not i ==''\n",
    "        except Exception as e:\n",
    "            print(tweet)\n",
    "            raise e\n",
    "        \n",
    "    return tweet_date, tweet_id, tweet_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* Opening the file \n",
    "* Removing the extra, stripping everything on the left and removing all the errors \n",
    "* Checking the language of the tweets if it is in english \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = {}\n",
    "discarded_tweets = []\n",
    "texts = []\n",
    "ids = []\n",
    "for file in text_files:\n",
    "    #Opening the file \n",
    "    with open(path + file, 'r', encoding='utf-8', errors='replace') as f:\n",
    "        text = f.read()      \n",
    "        #Removing the extra - Strippping everything on left - Removing all the errors \n",
    "        text = text.lstrip('{\"data:').split(',\"errors\":')[0][2:-2].split('},{')\n",
    "        for tweet in text:\n",
    "            tweet_date, tweet_id, tweet_text = parse_tweet(tweet)\n",
    "            tweet_text = clean_text(tweet_text) \n",
    "            lang, score = langid.classify(tweet_text)\n",
    "            #Checking the language if the tweets and if it is in english; appending\n",
    "            if lang == 'en':\n",
    "                if tweet_date not in tweets.keys():\n",
    "                    tweets[tweet_date] = []\n",
    "\n",
    "                tweets[tweet_date].append((tweet_id, tweet_text, score))\n",
    "                \n",
    "                texts.append(tweet_text)\n",
    "                ids.append(tweet_id)\n",
    "            else:\n",
    "                discarded_tweets.append((tweet_date, tweet_id, tweet_text, score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Writing it into xml file \n",
    "* Creating the xml file try.xml\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Writing it into xml file in xml format\n",
    "xmlfile_path = './29999715.xml'\n",
    "with open(xmlfile_path, 'w', encoding=\"UTF-8\") as f:\n",
    "        f.write(f\"\"\"<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n\"\"\")\n",
    "        f.write(\"<data>\\n\")\n",
    "        for date, date_tweets in tweets.items():\n",
    "            f.write(f\"\"\"<tweets date=\"{date}\">\\n\"\"\")\n",
    "            for tweet in date_tweets:\n",
    "                tweet_id, tweet_text = tweet[0], tweet[1]\n",
    "                f.write(f\"\"\"<tweet id=\"{tweet_id}\">{tweet_text}</tweet>\\n\"\"\")\n",
    "            f.write(f\"\"\"</tweets>\\n\"\"\")\n",
    "        f.write(\"</data>\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Verifying that the xml is loadable\n",
    "* Checking if the xml file is loadable "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xmltodict\n",
    "\n",
    "with open(xmlfile_path, 'r', encoding='utf-8') as f:  \n",
    "    xml_text = f.read()\n",
    "    \n",
    "parsed = xmltodict.parse(xml_text)['data']['tweets']\n",
    "output_texts = [] \n",
    "output_ids = []\n",
    "for p in parsed:\n",
    "    for i in p['tweet']:\n",
    "        output_texts.append(i['#text'])\n",
    "        output_ids.append(i['@id'])\n",
    "        \n",
    "assert len(texts) == len(output_texts)\n",
    "assert len(ids) == len(output_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Reference\n",
    "1. re — Regular expression operations — Python 3.8.6rc1 documentation. (2020). Retrieved 16 September 2020, from https://docs.python.org/3/library/re.html\n",
    "2. os — Miscellaneous operating system interfaces — Python 3.8.6rc1 documentation. (2020). Retrieved 16 September 2020, from https://docs.python.org/3/library/os.html\n",
    "3. langid. (2020). Retrieved 16 September 2020, from https://pypi.org/project/langid/1.1dev/\n",
    "4. xmltodict. (2020). Retrieved 16 September 2020, from https://pypi.org/project/xmltodict/\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
